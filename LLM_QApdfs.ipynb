{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c3ab93",
   "metadata": {},
   "source": [
    "# Document Q&A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015f708",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install openai\n",
    "\n",
    "# ! pip install pypdf \n",
    "# ! pip install langchain-community\n",
    "# ! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26c2a2a-f7a7-4580-ab73-178bab918dd7",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key  = os.getenv('HF_API_KEY')\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ef5d48",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "file_path = \"samplePDF.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd28c723-3625-4219-b0f8-8d5b761ae79e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ff4112",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "page = pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94e3b5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(page.page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d0932",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e549859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saraja\\AppData\\Local\\Temp\\ipykernel_10524\\1248922.py:5: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use Hugging Face embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "hidden layers to output. They are well-suited for tasks where sequential \n",
      "order is less relevant. CNNs also use the same structure as FFNN but they \n",
      "also include convolutional layers before hidden neurons that process \n",
      "fixed-size local receptive fields with automatic feature extraction, \n",
      "making them more suitable for grid-like data, e.g., images.\n",
      "Recurrent neural networks (RNNs) ( Elman, 1990 ; Rumelhart et al., \n",
      "1986 ; Werbos, 1988 ) are specialized DL models for tasks involving dy -\n",
      "namic temporal dependencies, such as natural language processing or \n",
      "time series forecasting ( Zou et al., 2023 ). RNNs have found extensive \n",
      "applications in various forecasting tasks. Compared to other models, \n",
      "RNNs are favored for their memory to handle time series tasks. Another \n",
      "distinguishing characteristic of RNNs is that they share parameters \n",
      "across each layer of the network, allowing the model to capture tem -\n",
      "poral patterns and dependencies. Compared to FFNNs which often as -\n",
      "sume inputs and outputs are independent of each other, RNNs consider \n",
      "one element at a time, allowing them to maintain memory through \n",
      "recurrent connections (see Fig. 2 ).\n",
      "RNNs, however, come with several drawbacks, specifically the \n",
      "challenge of exploding and vanishing gradients ( Bengio et al., 1993 ). \n",
      "During the training, RNNs use gradient descent to adjust their parame -\n",
      "ters (weights and biases) to minimize the prediction error. To accom -\n",
      "plish this, the RNN uses back-propagation through time that computes \n",
      "the gradients from the end of the sequence to the start. However, the \n",
      "multiplication of gradients in back-propagation, especially in sequences \n",
      "with many time steps, can lead to the exponential decay of gradients (i.e. \n",
      "the gradients approach zero), making them particularly susceptible to \n",
      "vanishing/exploding gradient problems ( Bengio et al., 1994 ). Accord -\n",
      "ingly, when gradients during backpropagation become extremely \n",
      "small/large, possible impacts can be: (i) hindering the algorithm from \n",
      "converging to an optimal solution, (ii) causing numerical instability \n",
      "during training, and (iii) making it challenging for the network to \n",
      "effectively learn long-term dependencies and adjust its parameters. \n",
      "These limitations lead to the development of LSTMs and GRUs (by \n",
      "introducing gating mechanisms), used in this research and will be \n",
      "explained in the following sub-sections.\n",
      "2.2.1. LSTM model\n",
      "The Long Short-Term Memory (LSTM) network emerges as a \n",
      "specialized solution and robust version of RNN models, which is \n",
      "particularly well-suited for the analysis of time series and sequential \n",
      "data, aimed at mitigating the long-term memory limitations that are \n",
      "inherent in conventional RNN networks ( Chen et al., 2021 ). A key \n",
      "feature of the LSTM network is its utilization of internal mechanisms \n",
      "known as gates. These gates regulate information transfer between the \n",
      "current and previous states, allowing them to learn both short-term and \n",
      "long-term dependencies in time series data ( Espinosa et al., 2021 ).\n",
      "Compared to RNN structure that only use one hidden recurrent layer, \n",
      "The architecture of LSTMs includes a memory cell that helps prevent the \n",
      "issues of gradient explosion or vanishing (see Fig. 3 ). Consequently, the \n",
      "LSTM network effectively propagates crucial information along the \n",
      "network to yield the desired output. The LSTM network presented in \n",
      "Fig. 3 (dotted lines) comprises three primary gates: the forget gate, input \n",
      "gate, and output gate, in addition to a cell state, explained below.\n",
      "Forget Gate: The function of the forget gate is to recognize the in -\n",
      "formation to be preserved and disregarded. It combines the input data of \n",
      "the current time step and the hidden state of the previous time step and \n",
      "then subjects this composite input to the sigmoid function. The output of \n",
      "this function, constrained between 0 and 1, signifies whether the in -\n",
      "formation should be retained (closer to 1) or forgotten (closer to 0). In \n",
      "the following Eqs. (3) – (6) , σ and Tanh are the sigmoid and hyperbolic \n",
      "tangent activation functions; ⋅ is the elementwise multiplication prod -\n",
      "uct; x\n",
      "t \n",
      "and h\n",
      "t \u0000 1 \n",
      "refer to the input at time t and the hidden state of the \n",
      "previous timestep, respectively; W\n",
      "∗\n",
      "and b\n",
      "∗\n",
      "show the weight and bias \n",
      "term associated with the gates; f\n",
      "t\n",
      ", i\n",
      "t\n",
      ", o\n",
      "t\n",
      "denote the forget, input, output of \n",
      "LSTM gates, respectively; and c\n",
      "t \n",
      "is the cell state ( He et al., 2023 ). \n",
      "f\n",
      "t\n",
      "= σ\n",
      "\u0000\n",
      "W\n",
      "f\n",
      "[ x\n",
      "t\n",
      ", h\n",
      "t \u0000 1\n",
      "] + b\n",
      "f\n",
      ")\n",
      "(3) \n",
      "Input Gate: The input gate is responsible for updating the values \n",
      "within the cell state. It receives the input data from the current time step \n",
      "and the hidden state information from the previous time step. These \n",
      "inputs undergo processing through the sigmoid function, determining \n",
      "which data should be updated (close to 1) or discarded (close to 0). \n",
      "Additionally, the input data from the current time step, coupled with the \n",
      "hidden state information from the previous time step, are subjected to \n",
      "the Tanh function, scaling their values within the range of \u0000 1 to 1. The \n",
      "output of the Sigmoid and Tanh functions is then multiplied to enable \n",
      "the Sigmoid function to decide which values of the Tanh function ’ s \n",
      "output should be retained. \n",
      "i\n",
      "t\n",
      "= σ ( W\n",
      "i\n",
      "[ x\n",
      "t\n",
      ", h\n",
      "t \u0000 1\n",
      "] + b\n",
      "i\n",
      ")\n",
      "̂c\n",
      "t\n",
      "= tanh ( W\n",
      "c\n",
      "[ x\n",
      "t\n",
      ", h\n",
      "t \u0000 1\n",
      "] + b\n",
      "c\n",
      ")\n",
      "(4) \n",
      "Cell State: The cell state is updated through a pointwise \n",
      "Fig. 3. Schematic architecture of GRU (dashed line), LSTM (dotted line) models and the proposed framework for spatio-temporal multivariate multistep ahead DT \n",
      "forecasting (solid line).\n",
      "S. Rajabi-Kiasari et al.                                                                                                                                                                                                                         Applied Ocean Research 157 (2025) 104496 \n",
      "6\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "horizons include 3, 6, 9, 12, 24 h ahead timesteps (parameters w and Δ \n",
      "in Eq. (2) ).\n",
      "5. Evaluation results\n",
      "5.1. General performance of the models\n",
      "The LSTM and GRU models ’ performance was assessed across \n",
      "various time horizons (3, 6, 9, 12, 24 h) using two metrics, RMSE and R\n",
      "2 \n",
      "as outlined in Eqs. (10) and (11) . Higher R\n",
      "2 \n",
      "values and lower RMSE \n",
      "values indicate a better fit of the model to the data. Based on the training \n",
      "results, for all three years averaged ( Table 4 ), the GRU model demon -\n",
      "strates a better performance for the training process with an averaged R\n",
      "2 \n",
      "of 0.94 and RMSE of 5.63 cm compared to the LSTM with 0.93 and 5.81 \n",
      "cm. Furthermore, for the test period, as shown in Fig. 7 and Table 5 , both \n",
      "GRU and LSTM models show good performance. This demonstrates the \n",
      "applied LSTM and GRU models ’ robustness.\n",
      "Results of Fig. 7 and Tables 4 and 5 demonstrate that the two models \n",
      "showed similar behavior given their comparable structures. As expected, \n",
      "the performance slightly deteriorates in both models as the time horizon \n",
      "increases. The GRU model, however, slightly outperformed LSTM with \n",
      "better results at all horizons, with higher R\n",
      "2 \n",
      "and lower RMSE ( Fig. 7 ). \n",
      "For instance, at 3 h horizon, the GRU model achieved an average RMSE \n",
      "of 3.55 cm, indicating that, on average, its predictions were approxi -\n",
      "mately 3.55 cm different from the true values, whilst the LSTM yields \n",
      "RMSE as of 4.13 cm. The GRU R\n",
      "2 \n",
      "value for this 3h was around 0.96, \n",
      "meaning the model explained 96 % of the variance in the data, \n",
      "demonstrating a strong predictive accuracy, whilst for the LSTM, it was \n",
      "0.95.\n",
      "At a 24-h horizon, both models ’ performance faced the most signif -\n",
      "icant challenge, the GRU had an average RMSE of 5.99 cm, whilst the \n",
      "RMSE for LSTM values was 6.17 cm. Both LSTM and GRU attained R\n",
      "2 \n",
      "of \n",
      "0.89 indicating that they still had a good overall fit to the data and could \n",
      "explain 89 % of the variance.\n",
      "Overall, both the LSTM and GRU models appear to be strong choices \n",
      "for sea level forecasting, providing almost similar and accurate forecasts \n",
      "within the specified time horizons. On average, the GRU model pro -\n",
      "duced R\n",
      "2 \n",
      "of 0.93 and a RMSE of 4.96 cm for daily predictions, while the \n",
      "LSTM model attained an R\n",
      "2 \n",
      "of 0.92 and an RMSE of 5.3 cm over the test \n",
      "data. So, the GRU model outperformed the LSTM model when consid -\n",
      "ering all time horizons.\n",
      "5.2. Models ’ time-series performance\n",
      "The temporal performance of the two models during the test period \n",
      "was also evaluated. This evaluation focused on the same four grid points \n",
      "(as depicted in Fig. 6 ) and generated time series plots for these locations, \n",
      "along with the predicted values provided by the LSTM and GRU models \n",
      "(see Fig. 8 ).\n",
      "Fig. 8 illustrates the comparison of sea-level time series forecasting \n",
      "results using LSTM and GRU models for 3 h and 12 h prediction horizons \n",
      "across four test points (P1 to P4). The GRU model includes a 90 % \n",
      "confidence interval (CI), and its reliability is assessed using Prediction \n",
      "Interval Coverage Probability (PICP) values (see Eq. (13) ). The PICP is a \n",
      "metric used here to assess the reliability of the model ’ s uncertainty es -\n",
      "timates. Both GRU and LSTM models generally follow the observed sea \n",
      "level trends well. However, the GRU model demonstrates slightly better \n",
      "accuracy in capturing rapid changes and variations, particularly for \n",
      "longer forecasting horizons like 12 h. The GRU ’ s 90 % CI effectively \n",
      "encompasses the observed values, as reflected in high PICP values \n",
      "exceeding 90 % in most cases. For example, at P1, the PICP is 92.17 % \n",
      "for the 3 h horizon and 90.71 % for the 12 h horizon, showing the GRU \n",
      "model ’ s strong reliability. The performance remains consistent across all \n",
      "points, although P2 at the 3-hour horizon has a slightly lower PICP of \n",
      "89.64 %, indicating marginally reduced robustness at that location. \n",
      "Similarly, for the 12-hour horizon, the PICP values for P1, P2, P3, and P4 \n",
      "range between 90.25 % and 91.02 %, all indicating strong performance \n",
      "in uncertainty quantification. These values reflect the model ’ s robust \n",
      "ability to maintain reliable and consistent predictions across different \n",
      "forecasting horizons, with the PICP consistently staying close to or above \n",
      "the target of 90 %. The GRU model ’ s confidence intervals widen for the \n",
      "12 h horizon compared to the 3 h horizon, reflecting the expected in -\n",
      "crease in forecast uncertainty over longer time frames. Despite this, the \n",
      "intervals still capture most of the observed values, demonstrating the \n",
      "model ’ s strong uncertainty quantification.\n",
      "However, for both models, there were instances where the results \n",
      "were less favorable, specifically for grid point P1 (mostly during Nov- \n",
      "Dec, when the sea level extremes occurred). Thus, both models appear \n",
      "to have problems with forecasting the extreme sea level values at all \n",
      "stations. Possible reasons for this will be discussed in more detail in \n",
      "Table 4 \n",
      "Training results of LSTM and GRU models.\n",
      "Model Train\n",
      "metric RMSE (cm) R\n",
      "2\n",
      "LSTM 5.81 0.93\n",
      "GRU 5.63 0.94\n",
      "Fig. 7. General performance of the LSTM, and GRU models during test period at different time horizons (3, 6, 9, 12, 24 h ahead) using spatially averaged R\n",
      "2 \n",
      "and RMSE.\n",
      "Table 5 \n",
      "Results of the performance of the DL models on different time horizons for the \n",
      "selected test period ( ’ 2019 – 07 – 21 to ’ 2019 – 12 – 30).\n",
      "Horizons (hours) Models\n",
      "GRU LSTM\n",
      "R\n",
      "2\n",
      "RMSE (cm) R\n",
      "2\n",
      "RMSE (cm)\n",
      "3 0.96 3.55 0.95 4.13\n",
      "6 0.95 4.41 0.94 4.85\n",
      "9 0.92 5.16 0.92 5.47\n",
      "12 0.91 5.67 0.90 5.87\n",
      "24 0.89 5.99 0.89 6.17\n",
      "average 0.93 4.96 0.92 5.3\n",
      "S. Rajabi-Kiasari et al.                                                                                                                                                                                                                         Applied Ocean Research 157 (2025) 104496 \n",
      "13\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "Wang, G., Wang, X., Wu, X., Liu, K., Qi, Y., Sun, C., Fu, H., 2022. A hybrid multivariate \n",
      "deep learning network for Multistep ahead sea level anomaly forecasting. J. Atmos. \n",
      "Ocean Technol. 39, 285 – 301. https://doi.org/10.1175/JTECH-D-21-0043.1 .\n",
      "Weisse, R., Dailidien ˙e, I., Hünicke, B., Kahma, K., Madsen, K., Omstedt, A., Parnell, K., \n",
      "Sch ¨one, T., Soomere, T., Zhang, W., Zorita, E., 2021. Sea level dynamics and coastal \n",
      "erosion in the Baltic Sea region. Earth Syst. Dyn. 12, 871 – 898. https://doi.org/ \n",
      "10.5194/esd-12-871-2021 .\n",
      "Werbos, P.J., 1988. Generalization of backpropagation with application to a recurrent \n",
      "gas market model. Neural Netw. 1, 339 – 356. https://doi.org/10.1016/0893-6080 \n",
      "(88)90007-X .\n",
      "Xiao, C., Chen, N., Hu, C., Wang, K., Xu, Z., Cai, Y., Xu, L., Chen, Z., Gong, J., 2019. \n",
      "A spatiotemporal deep learning model for sea surface temperature field prediction \n",
      "using time-series satellite data. Environ. Model. Softw. 120, 104502. https://doi. \n",
      "org/10.1016/j.envsoft.2019.104502 .\n",
      "Yu, H., Yang, L., Feng, Q., Barzegar, R., Adamowski, J.F., Wen, X., 2024. Ensemble \n",
      "learning of decomposition-based machine learning models for multistep-ahead daily \n",
      "streamflow forecasting in northwest China. Hydrol. Sci. J. 69 (11), 1501 – 1522 .\n",
      "Zhang, Z., Stanev, E.V., Grayek, S., 2020. Reconstruction of the basin-wide sea-level \n",
      "variability in the north sea using coastal data and generative adversarial networks. \n",
      "J. Geophys. Res. Oceans 125 (12), e2020JC016402 .\n",
      "Zhao, J., Cai, R., Sun, W., 2021. Regional sea level changes prediction integrated with \n",
      "singular spectrum analysis and long-short-term memory network. Adv. Space Res. \n",
      "68, 4534 – 4543. https://doi.org/10.1016/j.asr.2021.08.017 .\n",
      "Zhou, Y., Lu, C., Chen, K., Li, X., 2022. Multilayer fusion recurrent neural network for sea \n",
      "surface height anomaly field prediction. IEEE Trans. Geosci. Remote Sens. 60, 1 – 11. \n",
      "https://doi.org/10.1109/TGRS.2021.3126460 .\n",
      "Zilong, T., Yubing, S., Xiaowei, D., 2022. Spatial-temporal wave height forecast using \n",
      "deep learning and public reanalysis dataset. Appl. Energy 326, 120027. https://doi. \n",
      "org/10.1016/j.apenergy.2022.120027 .\n",
      "Zou, Y., Wang, J., Lei, P., Li, Y., 2023. A novel multi-step ahead forecasting model for \n",
      "flood based on time residual LSTM. J. Hydrol. 620, 129521. https://doi.org/ \n",
      "10.1016/j.jhydrol.2023.129521 .\n",
      "S. Rajabi-Kiasari et al.                                                                                                                                                                                                                         Applied Ocean Research 157 (2025) 104496 \n",
      "25\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "# Step 2: Create embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings()  # Use Hugging Face embeddings\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embeddings)\n",
    "\n",
    "# Step 3: function to query the PDF\n",
    "def query_pdf(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search the PDF for relevant content based on the query.\n",
    "    :param query: Your search query (e.g., \"What models are used?\")\n",
    "    :param top_k: Number of top results to return\n",
    "    :return: List of relevant document snippets\n",
    "    \"\"\"\n",
    "    # Perform similarity search\n",
    "    results = vectorstore.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Display the results\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Result {i + 1}:\")\n",
    "        print(result.page_content)  # Print the content of the relevant page\n",
    "        print(\"-\" * 80)  # Separator\n",
    "\n",
    "# Step 4: Query the PDF\n",
    "query = \"what Deep Learning model performed best?\"  # Replace with yours\n",
    "query_pdf(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f33ee",
   "metadata": {},
   "source": [
    "### improvements with larger huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab60328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: GRU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\HACKERRANK\\LLM\\llmvenv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "\n",
    "# Step 2: Split the text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Split text into chunks of 1000 characters\n",
    "    chunk_overlap=200,  # Add overlap to avoid losing context\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# Step 3: Create embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")  # Use a better embedding model\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(chunks, embeddings)\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# Access the Hugging Face API key\n",
    "hf_api_key = os.getenv('HF_API_KEY')\n",
    "\n",
    "# Step 4: Set up the retrieval-based QA chain\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-base\",  # Using a more powerful Hugging Face model\n",
    "    huggingfacehub_api_token=hf_api_key,  # Replace with your Hugging Face API key\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Use \"stuff\" for small documents\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),  # Retrieve top 5 chunks\n",
    ")\n",
    "\n",
    "# Step 5: Query the PDF sample which is an article\n",
    "# query = \"What models are used in this study?\"  # Replace with yours\n",
    "query = \"what Deep Learning model performed best?\"  # Replace with yours\n",
    "# query = \"what are the deep learning model input variables?\"  # Replace with yours\n",
    "# query = \"what is the aim of this study?\"  # Replace with yours\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "# Step 6: Display the response\n",
    "print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
